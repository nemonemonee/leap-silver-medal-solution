{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from models.datasets import *\n",
    "from models.fcn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = polars.read_csv('data/train.csv')\n",
    "test_df = polars.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.load('data/weight.pt')\n",
    "mask = w != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_COLS = df.columns[1:557]\n",
    "TGT_COLS = df.columns[557:]\n",
    "\n",
    "for col in SRC_COLS:\n",
    "    df = df.with_columns(polars.col(col).cast(polars.Float32))\n",
    "    test_df = test_df.with_columns(polars.col(col).cast(polars.Float32))\n",
    "\n",
    "for col in TGT_COLS:\n",
    "    df = df.with_columns(polars.col(col).cast(polars.Float32))\n",
    "\n",
    "\n",
    "src = torch.tensor(df.select(SRC_COLS).to_numpy(), dtype=torch.float32)\n",
    "test_src = torch.tensor(test_df.select(SRC_COLS).to_numpy(), dtype=torch.float32)\n",
    "label = torch.tensor(df.select(TGT_COLS).to_numpy(), dtype=torch.float32)\n",
    "src_mu = src.mean(axis=0)\n",
    "src_std = torch.maximum(src.std(axis=0), torch.tensor(1e-8))\n",
    "label_mu = label.mean(axis=0)\n",
    "label_std = torch.maximum(label.std(axis=0), torch.tensor(1e-8))\n",
    "test_src = (test_src - src_mu) / src_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq_name = [\n",
    "    \"state_t\", \"state_q0001\", \"state_q0002\", \"state_q0003\", \"state_u\", \"state_v\", \"pbuf_ozone\", \"pbuf_CH4\", \"pbuf_N2O\"\n",
    "]\n",
    "input_scl_name = [\n",
    "    \"state_ps\", \"pbuf_SOLIN\", \"pbuf_LHFLX\", \"pbuf_SHFLX\", \"pbuf_TAUX\", \"pbuf_TAUY\", \"pbuf_COSZRS\", \"cam_in_ALDIF\", \"cam_in_ALDIR\", \"cam_in_ASDIF\", \"cam_in_ASDIR\", \"cam_in_LWUP\", \"cam_in_ICEFRAC\", \"cam_in_LANDFRAC\", \"cam_in_OCNFRAC\", \"cam_in_SNOWHLAND\"\n",
    "]\n",
    "input_seq_idx = [[idx - 1 for idx, column in enumerate(df.columns) if \n",
    "                    column.startswith(var)] for var in input_seq_name]\n",
    "input_scl_idx = [[idx - 1 for idx, column in enumerate(df.columns) if \n",
    "                    column.startswith(var)] for var in input_scl_name]\n",
    "test_seq = torch.stack([test_src[:, i] for i in input_seq_idx], dim=-1)\n",
    "test_scl = torch.stack([test_src[:, i].repeat(1, 60) for i in input_scl_idx], dim=-1)\n",
    "input_dim = test_seq.size(-1)\n",
    "scalar_dim = test_scl.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = MixTestDataset(test_seq, test_scl)\n",
    "test_loader = DataLoader(test_ds, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 8 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=8)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "ckpt = \"/m9400/users/lkv6309/leap/ckpt/fnc-epoch=332-val_loss=0.195.ckpt\"\n",
    "model = AttentionFCN.load_from_checkpoint(ckpt)\n",
    "# model = TransformerFilteredModel.load_from_checkpoint(ckpt)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='ckpt/',\n",
    "    filename='te-base-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=-1,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")\n",
    "logger = TensorBoardLogger(save_dir=\"logger\")\n",
    "trainer = Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=10,\n",
    "    accelerator=\"gpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA H100 80GB HBM3') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "/m9400/users/lkv6309/miniconda3/envs/rise/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=111` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b92fdfa5ec415ca9adad410de3ab77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = trainer.predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = \"/m9400/users/lkv6309/leap/ckpt/tf-epoch=111-val_loss=0.32.ckpt\"\n",
    "# model2 = TransformerFilteredModel.load_from_checkpoint(ckpt)\n",
    "# # model = TransformerFilteredModel.load_from_checkpoint(ckpt)\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     dirpath='ckpt/',\n",
    "#     filename='te-base-{epoch:02d}-{val_loss:.2f}',\n",
    "#     save_top_k=-1,\n",
    "#     monitor='val_loss',\n",
    "#     mode='min'\n",
    "# )\n",
    "# logger = TensorBoardLogger(save_dir=\"logger\")\n",
    "# trainer2 = Trainer(\n",
    "#     logger=logger,\n",
    "#     callbacks=[checkpoint_callback],\n",
    "#     max_epochs=10,\n",
    "#     accelerator=\"gpu\",\n",
    "# )\n",
    "# pred2 = trainer2.predict(model2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.cat(pred)\n",
    "for i in range(label_std.shape[0]):\n",
    "    if label_std[i] < 1.1e-8:\n",
    "        preds[:,i] = 0\n",
    "preds = (preds * label_std + label_mu)  * w\n",
    "# preds2 = torch.cat(pred2)\n",
    "# for i in range(label_std.shape[0]):\n",
    "#     if label_std[i] < 1.1e-8:\n",
    "#         preds2[:,i] = 0\n",
    "# preds = ((preds+preds2)/2 * label_std + label_mu)  * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8005)\n"
     ]
    }
   ],
   "source": [
    "print(preds[:,mask].std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.9851e-01, -5.7869e-01, -3.9847e-01, -6.5252e-01, -9.1345e-01,\n",
      "        -9.9535e-01, -9.7802e-01, -1.0455e+00, -1.0773e+00, -1.0485e+00,\n",
      "        -9.9329e-01, -9.4116e-01, -9.2869e-01, -7.9476e-01, -3.1186e-01,\n",
      "         2.8042e-01,  5.0008e-01,  1.0036e-01,  3.7329e-01, -2.0165e-01,\n",
      "        -1.6992e-01, -3.8444e-01, -5.3666e-01, -5.8915e-01, -5.5097e-01,\n",
      "        -5.3382e-01, -5.7886e-01, -6.1382e-01, -6.1019e-01, -6.0842e-01,\n",
      "        -5.8934e-01, -5.1812e-01, -4.8346e-01, -4.1892e-01, -3.9040e-01,\n",
      "        -4.0021e-01, -3.6307e-01, -3.6683e-01, -3.7326e-01, -3.6886e-01,\n",
      "        -3.4299e-01, -3.3935e-01, -3.6308e-01, -4.6040e-01, -3.7759e-01,\n",
      "        -4.2426e-01, -4.0262e-01, -5.1220e-01, -4.3263e-01, -4.4767e-01,\n",
      "        -5.6420e-01, -7.8237e-01, -7.7378e-01, -8.2859e-01, -6.4627e-01,\n",
      "        -5.2525e-01, -5.0269e-01, -5.2644e-01, -4.3785e-01, -3.1730e-01,\n",
      "        -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  2.5804e-03,  8.1679e-03,  1.4571e-03,\n",
      "         1.7086e-03,  1.1663e-03, -1.1792e-02, -1.4479e-02, -2.8916e-02,\n",
      "        -5.1120e-02, -7.6257e-02, -1.0091e-01, -1.2218e-01, -1.4171e-01,\n",
      "        -1.5780e-01, -1.6872e-01, -1.7413e-01,  2.5400e-02,  6.7930e-02,\n",
      "         9.7932e-02,  6.5489e-02,  5.8686e-02,  1.5057e-02,  1.4642e-02,\n",
      "         3.9374e-02,  7.2619e-03,  1.6140e-02,  2.0780e-02,  1.5555e-02,\n",
      "        -3.0839e-03,  6.1716e-04,  1.9996e-02,  7.0538e-02,  4.2706e-02,\n",
      "         4.1517e-02,  3.0336e-02,  7.0464e-02,  3.2530e-02,  3.8038e-02,\n",
      "         8.1484e-02,  1.7131e-01,  3.0599e-01,  3.0466e-01, -4.5072e-02,\n",
      "        -1.5555e-01, -6.1419e-02, -5.1038e-02,  6.1926e-03, -4.0816e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -3.0408e-28, -1.0144e-25,\n",
      "        -6.4649e-23, -4.5027e-20, -3.6527e-17, -2.8982e-14, -6.0708e-11,\n",
      "        -1.1158e-06, -5.1440e-04, -2.8901e-03, -4.8879e-02, -9.4945e-02,\n",
      "        -1.1570e-01, -8.5156e-02, -4.3942e-02, -2.5610e-02, -1.7544e-02,\n",
      "        -1.0952e-02, -5.4306e-03, -2.6981e-03, -9.6453e-04,  4.0226e-04,\n",
      "         1.5084e-03,  3.4653e-03,  4.7904e-03,  6.3037e-03,  8.0743e-03,\n",
      "         9.7055e-03, -1.9344e-01, -4.1708e-01,  1.0046e-02,  1.0539e-02,\n",
      "         1.3077e-02,  1.6108e-02,  1.8410e-02,  1.7537e-02,  1.4752e-02,\n",
      "         1.0459e-02,  7.0027e-03,  1.6142e-03, -1.6309e-02, -2.0997e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -7.5057e-04, -8.6545e-04, -1.0428e-03,\n",
      "         8.0365e-05, -5.2346e-05, -7.5775e-04, -4.1692e-03, -6.8708e-03,\n",
      "        -1.1320e-02, -1.5115e-02, -1.7059e-02, -1.7385e-02, -1.8043e-02,\n",
      "        -1.9243e-02, -1.9875e-02, -1.8058e-02, -1.3912e-02, -6.1028e-03,\n",
      "         5.5135e-03,  1.5039e-02,  1.8458e-02,  1.8528e-02,  1.8839e-02,\n",
      "         1.9949e-02,  1.9291e-02,  1.6667e-02,  1.5043e-02,  1.4618e-02,\n",
      "         1.3886e-02,  1.2609e-02,  1.1270e-02,  1.0523e-02,  1.0169e-02,\n",
      "         9.8548e-03,  9.7068e-03,  1.1387e-02,  1.4747e-02,  1.9341e-02,\n",
      "         2.2726e-02,  2.4135e-02,  2.2610e-02,  1.7523e-02,  8.6268e-03,\n",
      "        -2.0526e-03, -1.1486e-02, -1.6561e-02, -3.5931e-02, -1.6036e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.3594e-02,  3.2969e-03, -1.4381e-05,\n",
      "        -9.8162e-03, -1.6707e-03,  1.0833e-02,  2.0188e-02,  5.9651e-04,\n",
      "         1.5081e-02,  3.0677e-01, -4.1798e-01,  1.1155e-01, -3.8996e-02,\n",
      "        -2.3996e-02, -1.1968e-02, -1.7263e-02, -3.1259e-02,  6.0304e-03,\n",
      "         2.5598e-02,  7.5626e-03, -6.1889e-03,  9.2234e-03,  1.2990e-02,\n",
      "         4.1569e-03,  1.2213e-03,  6.1184e-03,  4.0065e-03,  3.1788e-03,\n",
      "        -3.9008e-03, -3.8935e-03,  1.0802e-03,  1.2030e-02,  7.4032e-03,\n",
      "        -1.9888e-02,  2.9812e-04,  1.0404e-02,  1.9215e-02,  2.4799e-02,\n",
      "         1.5187e-02, -1.8173e-02, -3.7449e-02, -7.1654e-02, -1.7308e-02,\n",
      "         1.9133e-02,  5.6832e-02,  1.2072e-01,  1.0787e-01, -1.1273e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.7829e+00, -3.8181e-02, -1.6363e+00,\n",
      "         4.9560e-01, -7.6165e-02,  8.3759e-03, -5.2373e-02,  1.2687e-01,\n",
      "        -1.1818e-01, -1.4112e-01,  1.8036e-01, -3.6804e-02, -6.9947e-02,\n",
      "         2.1915e-02,  3.7298e-03, -1.1472e-02, -1.0067e-01,  1.6062e-01,\n",
      "        -1.4194e-02, -3.9602e-02, -5.0720e-03,  4.0012e-03, -1.4125e-01,\n",
      "         4.7085e-02,  1.6373e-02,  8.3568e-03,  1.2866e-02, -3.1147e-03,\n",
      "         9.2341e-03, -4.6792e-02, -1.5446e-02, -1.2546e-02,  6.9004e-02,\n",
      "         1.4188e-03,  3.2999e-03,  1.7554e-03,  4.2885e-03, -2.2177e-03,\n",
      "        -1.7866e-03,  3.6171e-03,  6.5961e-03,  3.2080e-03, -1.2124e-02,\n",
      "        -2.7945e-02, -1.0849e-02, -3.1254e-02, -2.6949e-01,  2.0870e-01,\n",
      "         5.7480e-03,  5.3878e+00,  3.7008e-01, -1.9582e-02,  1.2266e-02,\n",
      "        -7.9593e-04,  1.0963e-02, -2.7499e-02])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3716)\n",
      "tensor(0.3923)\n"
     ]
    }
   ],
   "source": [
    "print(0.24 * mask.sum() / torch.logical_and(mask, label_std > 1.1e-8).sum())\n",
    "print(0.21 * len(mask) / torch.logical_and(mask, label_std > 1.1e-8).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.9851e-01, -5.7869e-01, -3.9847e-01, -6.5252e-01, -9.1345e-01,\n",
      "        -9.9535e-01, -9.7802e-01, -1.0455e+00, -1.0773e+00, -1.0485e+00,\n",
      "        -9.9329e-01, -9.4116e-01, -9.2869e-01, -7.9476e-01, -3.1186e-01,\n",
      "         2.8042e-01,  5.0008e-01,  1.0036e-01,  3.7329e-01, -2.0165e-01,\n",
      "        -1.6992e-01, -3.8444e-01, -5.3666e-01, -5.8915e-01, -5.5097e-01,\n",
      "        -5.3382e-01, -5.7886e-01, -6.1382e-01, -6.1019e-01, -6.0842e-01,\n",
      "        -5.8934e-01, -5.1812e-01, -4.8346e-01, -4.1892e-01, -3.9040e-01,\n",
      "        -4.0021e-01, -3.6307e-01, -3.6683e-01, -3.7326e-01, -3.6886e-01,\n",
      "        -3.4299e-01, -3.3935e-01, -3.6308e-01, -4.6040e-01, -3.7759e-01,\n",
      "        -4.2426e-01, -4.0262e-01, -5.1220e-01, -4.3263e-01, -4.4767e-01,\n",
      "        -5.6420e-01, -7.8237e-01, -7.7378e-01, -8.2859e-01, -6.4627e-01,\n",
      "        -5.2525e-01, -5.0269e-01, -5.2644e-01, -4.3785e-01, -3.1730e-01,\n",
      "        -0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  2.5804e-03,  8.1679e-03,  1.4571e-03,\n",
      "         1.7086e-03,  1.1663e-03, -1.1792e-02, -1.4479e-02, -2.8916e-02,\n",
      "        -5.1120e-02, -7.6257e-02, -1.0091e-01, -1.2218e-01, -1.4171e-01,\n",
      "        -1.5780e-01, -1.6872e-01, -1.7413e-01,  2.5400e-02,  6.7930e-02,\n",
      "         9.7932e-02,  6.5489e-02,  5.8686e-02,  1.5057e-02,  1.4642e-02,\n",
      "         3.9374e-02,  7.2619e-03,  1.6140e-02,  2.0780e-02,  1.5555e-02,\n",
      "        -3.0839e-03,  6.1716e-04,  1.9996e-02,  7.0538e-02,  4.2706e-02,\n",
      "         4.1517e-02,  3.0336e-02,  7.0464e-02,  3.2530e-02,  3.8038e-02,\n",
      "         8.1484e-02,  1.7131e-01,  3.0599e-01,  3.0466e-01, -4.5072e-02,\n",
      "        -1.5555e-01, -6.1419e-02, -5.1038e-02,  6.1926e-03, -4.0816e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -3.0408e-28, -1.0144e-25,\n",
      "        -6.4649e-23, -4.5027e-20, -3.6527e-17, -2.8982e-14, -6.0708e-11,\n",
      "        -1.1158e-06, -5.1440e-04, -2.8901e-03, -4.8879e-02, -9.4945e-02,\n",
      "        -1.1570e-01, -8.5156e-02, -4.3942e-02, -2.5610e-02, -1.7544e-02,\n",
      "        -1.0952e-02, -5.4306e-03, -2.6981e-03, -9.6453e-04,  4.0226e-04,\n",
      "         1.5084e-03,  3.4653e-03,  4.7904e-03,  6.3037e-03,  8.0743e-03,\n",
      "         9.7055e-03, -1.9344e-01, -4.1708e-01,  1.0046e-02,  1.0539e-02,\n",
      "         1.3077e-02,  1.6108e-02,  1.8410e-02,  1.7537e-02,  1.4752e-02,\n",
      "         1.0459e-02,  7.0027e-03,  1.6142e-03, -1.6309e-02, -2.0997e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -7.5057e-04, -8.6545e-04, -1.0428e-03,\n",
      "         8.0365e-05, -5.2346e-05, -7.5775e-04, -4.1692e-03, -6.8708e-03,\n",
      "        -1.1320e-02, -1.5115e-02, -1.7059e-02, -1.7385e-02, -1.8043e-02,\n",
      "        -1.9243e-02, -1.9875e-02, -1.8058e-02, -1.3912e-02, -6.1028e-03,\n",
      "         5.5135e-03,  1.5039e-02,  1.8458e-02,  1.8528e-02,  1.8839e-02,\n",
      "         1.9949e-02,  1.9291e-02,  1.6667e-02,  1.5043e-02,  1.4618e-02,\n",
      "         1.3886e-02,  1.2609e-02,  1.1270e-02,  1.0523e-02,  1.0169e-02,\n",
      "         9.8548e-03,  9.7068e-03,  1.1387e-02,  1.4747e-02,  1.9341e-02,\n",
      "         2.2726e-02,  2.4135e-02,  2.2610e-02,  1.7523e-02,  8.6268e-03,\n",
      "        -2.0526e-03, -1.1486e-02, -1.6561e-02, -3.5931e-02, -1.6036e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -1.3594e-02,  3.2969e-03, -1.4381e-05,\n",
      "        -9.8162e-03, -1.6707e-03,  1.0833e-02,  2.0188e-02,  5.9651e-04,\n",
      "         1.5081e-02,  3.0677e-01, -4.1798e-01,  1.1155e-01, -3.8996e-02,\n",
      "        -2.3996e-02, -1.1968e-02, -1.7263e-02, -3.1259e-02,  6.0304e-03,\n",
      "         2.5598e-02,  7.5626e-03, -6.1889e-03,  9.2234e-03,  1.2990e-02,\n",
      "         4.1569e-03,  1.2213e-03,  6.1184e-03,  4.0065e-03,  3.1788e-03,\n",
      "        -3.9008e-03, -3.8935e-03,  1.0802e-03,  1.2030e-02,  7.4032e-03,\n",
      "        -1.9888e-02,  2.9812e-04,  1.0404e-02,  1.9215e-02,  2.4799e-02,\n",
      "         1.5187e-02, -1.8173e-02, -3.7449e-02, -7.1654e-02, -1.7308e-02,\n",
      "         1.9133e-02,  5.6832e-02,  1.2072e-01,  1.0787e-01, -1.1273e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.7829e+00, -3.8181e-02, -1.6363e+00,\n",
      "         4.9560e-01, -7.6165e-02,  8.3759e-03, -5.2373e-02,  1.2687e-01,\n",
      "        -1.1818e-01, -1.4112e-01,  1.8036e-01, -3.6804e-02, -6.9947e-02,\n",
      "         2.1915e-02,  3.7298e-03, -1.1472e-02, -1.0067e-01,  1.6062e-01,\n",
      "        -1.4194e-02, -3.9602e-02, -5.0720e-03,  4.0012e-03, -1.4125e-01,\n",
      "         4.7085e-02,  1.6373e-02,  8.3568e-03,  1.2866e-02, -3.1147e-03,\n",
      "         9.2341e-03, -4.6792e-02, -1.5446e-02, -1.2546e-02,  6.9004e-02,\n",
      "         1.4188e-03,  3.2999e-03,  1.7554e-03,  4.2885e-03, -2.2177e-03,\n",
      "        -1.7866e-03,  3.6171e-03,  6.5961e-03,  3.2080e-03, -1.2124e-02,\n",
      "        -2.7945e-02, -1.0849e-02, -3.1254e-02, -2.6949e-01,  2.0870e-01,\n",
      "         5.7480e-03,  5.3878e+00,  3.7008e-01, -1.9582e-02,  1.2266e-02,\n",
      "        -7.9593e-04,  1.0963e-02, -2.7499e-02])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = pd.read_csv(\"sample_submission.csv\")\n",
    "ss.iloc[:,1:] = preds.numpy()\n",
    "test_polars = polars.from_pandas(ss[[\"sample_id\"]+TGT_COLS])\n",
    "test_polars.write_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /m9400/users/lkv6309/.kaggle/kaggle.json'\n",
      "100%|███████████████████████████████████████| 3.73G/3.73G [00:34<00:00, 116MB/s]\n",
      "Successfully submitted to LEAP - Atmospheric Physics using AI (ClimSim)"
     ]
    }
   ],
   "source": [
    "! kaggle competitions submit -c leap-atmospheric-physics-ai-climsim -f submission.csv -m \"fcn val 0.20\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
